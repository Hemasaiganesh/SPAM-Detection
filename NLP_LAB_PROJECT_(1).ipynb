# Install required packages (Colab)
!pip install -q wordcloud xgboost joblib

# ===== spam_detection_colab.py =====
"""
Spam Detection - end-to-end script for Colab / local use.

Steps:
1. Place 'spam.csv' in the working directory (or upload to Colab).
2. Run this script. It trains classifiers, prints evaluation, and saves:
   - best_model.pkl (best performing classifier)
   - tfidf_vectorizer.pkl
"""

# Standard imports
import os
import string
from collections import Counter

import matplotlib.pyplot as plt
import nltk
import numpy as np
import pandas as pd
from wordcloud import WordCloud

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split

# Classifiers
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

import xgboost as xgb
import joblib

# Ensure required NLTK data
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# -------- Configuration --------
DATA_PATH = "spam.csv"         # CSV file (from your dataset)
RANDOM_STATE = 111
TEST_SIZE = 0.15
SAVE_DIR = "."                # where to save models

# -------- Utility functions --------
def load_and_clean_csv(path):
    """Load dataset and cleanup columns (for the common spam.csv format)."""
    df = pd.read_csv(path, encoding='latin-1')
    # Drop unused columns if present
    drop_cols = [c for c in df.columns if c.startswith("Unnamed")]
    df = df.drop(columns=drop_cols, errors='ignore')
    # Rename columns if v1/v2 present
    if 'v1' in df.columns and 'v2' in df.columns:
        df = df.rename(columns={'v1': 'label', 'v2': 'text'})
    df = df[['label', 'text']].dropna().reset_index(drop=True)
    return df

def preprocess_text(text):
    """Basic preprocessing: remove punctuation, remove stopwords (english)."""
    # remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # tokenize
    tokens = text.split()
    # remove stopwords
    tokens = [w for w in tokens if w.lower() not in stopwords.words('english')]
    return " ".join(tokens)

def create_wordclouds(df):
    spam_words = ""
    ham_words = ""
    for val in df[df['label'] == 1]['text']:
        tokens = word_tokenize(val.lower())
        spam_words += " ".join(tokens) + " "
    for val in df[df['label'] == 0]['text']:
        tokens = word_tokenize(val.lower())
        ham_words += " ".join(tokens) + " "

    spam_wc = WordCloud(width=600, height=300).generate(spam_words)
    ham_wc = WordCloud(width=600, height=300).generate(ham_words)

    plt.figure(figsize=(12,5))
    plt.subplot(1,2,1); plt.imshow(spam_wc); plt.axis('off'); plt.title('Spam WordCloud')
    plt.subplot(1,2,2); plt.imshow(ham_wc); plt.axis('off'); plt.title('Ham WordCloud')
    plt.show()

def evaluate_classifier(name, clf, X_test, y_test):
    pred = clf.predict(X_test)
    acc = accuracy_score(y_test, pred)
    print(f"--- {name} ---")
    print("Accuracy:", round(acc, 4))
    print("Classification Report:")
    print(classification_report(y_test, pred, digits=4))
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, pred))
    print()
    return acc

# -------- Main pipeline --------
def main():
    if not os.path.exists(DATA_PATH):
        raise FileNotFoundError(f"{DATA_PATH} not found. Upload it to the working directory.")

    # Load
    data = load_and_clean_csv(DATA_PATH)
    print("Loaded dataset shape:", data.shape)
    print(data.head())

    # Map labels to 0/1 (ham=0, spam=1)
    data['label'] = data['label'].map({'ham': 0, 'spam': 1}).astype(int)

    # Basic EDA: message lengths
    data['length'] = data['text'].astype(str).apply(len)
    plt.figure(figsize=(8,5))
    data['length'].hist(bins=50)
    plt.title("Message length distribution")
    plt.show()

    # Word clouds (optional)
    try:
        create_wordclouds(data)
    except Exception as e:
        print("WordCloud generation failed (maybe missing fonts). Continuing.")

    # Preprocess text column
    print("Preprocessing text...")
    data['text'] = data['text'].astype(str).apply(preprocess_text)

    # Quick vocab stats (optional)
    total_counts = Counter()
    for text in data['text']:
        for w in text.split():
            total_counts[w] += 1
    print("Total unique words:", len(total_counts))

    # TF-IDF vectorization
    print("Vectorizing with TF-IDF...")
    tfidf = TfidfVectorizer()
    X = tfidf.fit_transform(data['text'])
    y = data['label'].values
    print("Feature matrix shape:", X.shape)

    # train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE,
                                                        random_state=RANDOM_STATE, stratify=y)
    print("Train shape:", X_train.shape, "Test shape:", X_test.shape)

    # Define classifiers (you can tune hyperparams)
    svc = SVC(kernel='sigmoid', gamma=1.0, probability=True)
    knc = KNeighborsClassifier(n_neighbors=9)
    mnb = MultinomialNB(alpha=0.2)
    dtc = DecisionTreeClassifier(min_samples_split=7, random_state=RANDOM_STATE)
    lrc = LogisticRegression(solver='liblinear', penalty='l1', random_state=RANDOM_STATE)
    rfc = RandomForestClassifier(n_estimators=31, random_state=RANDOM_STATE)

    clfs = {
        'Multinomial Naive Bayes': mnb,
        'Logistic Regression': lrc,
        'Random Forest': rfc,
        'Decision Tree': dtc,
        'KNN': knc,
        'SVC': svc
    }

    # Train and evaluate classical classifiers
    results = {}
    for name, clf in clfs.items():
        print(f"Training {name} ...")
        clf.fit(X_train, y_train)
        acc = evaluate_classifier(name, clf, X_test, y_test)
        results[name] = (acc, clf)

    # Train XGBoost (uses DMatrix)
    print("Training XGBoost ...")
    params = {
        'objective': 'binary:logistic',
        'eval_metric': 'error',
        'eta': 0.02,
        'max_depth': 8,
        'verbosity': 0
    }
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dtest = xgb.DMatrix(X_test, label=y_test)
    xgb_model = xgb.train(params, dtrain, num_boost_round=200, verbose_eval=False)
    # Predict (probabilities)
    xgb_preds = (xgb_model.predict(dtest) >= 0.5).astype(int)
    xgb_acc = accuracy_score(y_test, xgb_preds)
    print("XGBoost Accuracy:", round(xgb_acc, 4))
    print(classification_report(y_test, xgb_preds, digits=4))
    results['XGBoost'] = (xgb_acc, xgb_model)

    # Select best model by accuracy
    best_name, best_acc = None, -1
    best_model = None
    for name, (acc, model) in results.items():
        if acc > best_acc:
            best_acc = acc
            best_name = name
            best_model = model

    print(f"Best model: {best_name} with accuracy {best_acc:.4f}")

    # Save model + vectorizer. For XGBoost model special handling
    if best_name == 'XGBoost':
        # save xgb using its native method
        xgb_model.save_model(os.path.join(SAVE_DIR, 'best_xgb.model'))
        # Save a lightweight wrapper to indicate xgb is the best
        joblib.dump({'model_type': 'xgboost', 'model_path': os.path.join(SAVE_DIR, 'best_xgb.model')},
                    os.path.join(SAVE_DIR, 'best_model_metadata.pkl'))
        print("Saved XGBoost model and metadata.")
    else:
        # scikit-learn model
        joblib.dump(best_model, os.path.join(SAVE_DIR, 'best_model.pkl'))
        print("Saved best model as best_model.pkl")

    # Save vectorizer (TFIDF)
    joblib.dump(tfidf, os.path.join(SAVE_DIR, 'tfidf_vectorizer.pkl'))
    print("Saved TF-IDF vectorizer as tfidf_vectorizer.pkl")

    # Helper predict function demonstration (load and predict)
    print("\nDemo predictions using MultinomialNB (if available) or the best saved model...")
    demo_texts = [
        "Free tones Hope you enjoyed your new content",
        "That I'll call later",
        "WINNER!! You just won a free ticket to Bahamas. Send your Details"
    ]
    vect = tfidf.transform(demo_texts)

    # If best model is xgboost, load it using xgboost.Booster
    if best_name == 'XGBoost':
        loaded = xgb.Booster()
        loaded.load_model(os.path.join(SAVE_DIR, 'best_xgb.model'))
        dvec = xgb.DMatrix(vect)
        preds = (loaded.predict(dvec) >= 0.5).astype(int)
    else:
        loaded = joblib.load(os.path.join(SAVE_DIR, 'best_model.pkl'))
        preds = loaded.predict(vect)

    for t, p in zip(demo_texts, preds):
        label = "SPAM" if int(p) == 1 else "NOT SPAM"
        print(f"[{label}] {t}")

    print("\nScript finished.")

if __name__ == "__main__":
    main()

# ===== end of script =====
